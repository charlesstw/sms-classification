{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWBvabbRT_BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "4.53.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0138dc56cb4ba297dfb7f9ffcbeaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e193de20d87148faa160f45e0171863c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 07:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 評估結果：\n",
      "eval_loss: 0.0087\n",
      "eval_accuracy: 0.9979\n",
      "eval_precision: 0.9954\n",
      "eval_recall: 1.0000\n",
      "eval_f1: 0.9977\n",
      "eval_runtime: 9.8179\n",
      "eval_samples_per_second: 48.3810\n",
      "eval_steps_per_second: 6.1110\n",
      "epoch: 3.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/final/ (stored 0%)\n",
      "  adding: results/final/model.safetensors (deflated 7%)\n",
      "  adding: results/final/tokenizer_config.json (deflated 75%)\n",
      "  adding: results/final/special_tokens_map.json (deflated 42%)\n",
      "  adding: results/final/config.json (deflated 54%)\n",
      "  adding: results/final/tokenizer.json (deflated 75%)\n",
      "  adding: results/final/vocab.txt (deflated 48%)\n",
      "已完成訓練\n"
     ]
    }
   ],
   "source": [
    "# 📌 安裝必要套件（Colab 每次都需要執行）\n",
    "# !pip install -U transformers datasets\n",
    "\n",
    "# 📌 匯入必要套件\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "\n",
    "# 顯示 transformers 套件版本，確認版本相容性\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# 📌 載入簡訊資料集（請確認檔案已上傳）\n",
    "df = pd.read_csv(\"train/train_data_0711_2.csv\")  # 載入包含 'text' 和 'label' 欄位的 CSV 檔案\n",
    "\n",
    "# 將 pandas DataFrame 轉成 Hugging Face 的 Dataset 格式\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "\n",
    "# 將資料集切分成訓練集與測試集，比例為 80% 訓練、20% 測試\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# 📌 初始化 tokenizer（使用 bert-base-chinese）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 📌 定義 tokenize 函式：將每則簡訊轉換成 BERT 模型可以讀懂的格式\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,              # 若文字太長會截斷\n",
    "        padding=\"max_length\",         # 補齊到固定長度（128）\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 對資料集進行 tokenize，產生 token 編碼、attention mask 等欄位\n",
    "tokenized = dataset.map(tokenize)\n",
    "\n",
    "# 📌 載入 BERT 模型做二分類（label 有兩種可能）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=2                    # 二分類：例如 是否包含人名 / 詐騙等\n",
    ")\n",
    "\n",
    "# 📌 定義訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # 模型與檔案輸出目錄\n",
    "    per_device_train_batch_size=16,         # 每個設備的訓練批次大小\n",
    "    num_train_epochs=3,                     # 訓練週期數\n",
    "    eval_strategy=\"epoch\",                 # 每個 epoch 評估一次測試集\n",
    "    save_strategy=\"epoch\",                 # 每個 epoch 儲存一次模型\n",
    "    logging_dir=\"./logs\",                  # 訓練過程 log 輸出資料夾\n",
    "    load_best_model_at_end=True,           # 訓練結束時載入最好的模型（根據 eval loss）\n",
    "    report_to=\"none\"                       # 不上傳訓練記錄到 wandb\n",
    ")\n",
    "\n",
    "# 📌 定義指標計算函式（accuracy, precision, recall, F1）\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 模型預測分數 和 真實標籤\n",
    "    preds = logits.argmax(axis=1)  # 取最大分數的類別作為預測\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')  # 2分類任務\n",
    "    acc = accuracy_score(labels, preds)  # 計算準確率\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# 📌 初始化 Trainer 物件：整合模型、資料與訓練參數\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],      # 訓練集\n",
    "    eval_dataset=tokenized[\"test\"],         # 測試集\n",
    "    compute_metrics=compute_metrics        # 加上這個函式才能顯示準確率與 F1\n",
    ")\n",
    "\n",
    "# 📌 執行模型訓練\n",
    "trainer.train()\n",
    "\n",
    "# 📌 使用測試集進行評估\n",
    "results = trainer.evaluate()\n",
    "print(\"📊 評估結果：\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# 📌 儲存訓練完成的模型與 tokenizer（方便未來部署或載入使用）\n",
    "model.save_pretrained(\"./results/final\")\n",
    "tokenizer.save_pretrained(\"./results/final\")\n",
    "\n",
    "# 📌 將儲存的模型打包成壓縮檔，方便下載\n",
    "!zip -r model_results.zip ./results/final\n",
    "\n",
    "print(\"已完成訓練\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP70pwkffXXTy0PTlKZzhAc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
