{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iWBvabbRT_BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "4.53.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cf620881754380b96c7b113b7edad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d98cc0f494680b406ba54a2fbfc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='524' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [524/786 10:37 < 05:19, 0.82 it/s, Epoch 4/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.121784</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.976645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.031574</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.995671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.060055</td>\n",
       "      <td>0.992337</td>\n",
       "      <td>0.987124</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.991379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.039119</td>\n",
       "      <td>0.994253</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.993521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è©•ä¼°çµæœï¼š\n",
      "eval_loss: 0.0316\n",
      "eval_accuracy: 0.9962\n",
      "eval_precision: 0.9957\n",
      "eval_recall: 0.9957\n",
      "eval_f1: 0.9957\n",
      "eval_runtime: 10.2892\n",
      "eval_samples_per_second: 50.7330\n",
      "eval_steps_per_second: 6.4150\n",
      "epoch: 4.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/final/ (stored 0%)\n",
      "  adding: results/final/model.safetensors (deflated 7%)\n",
      "  adding: results/final/tokenizer_config.json (deflated 75%)\n",
      "  adding: results/final/special_tokens_map.json (deflated 42%)\n",
      "  adding: results/final/config.json (deflated 54%)\n",
      "  adding: results/final/tokenizer.json (deflated 75%)\n",
      "  adding: results/final/vocab.txt (deflated 48%)\n",
      "å·²å®Œæˆè¨“ç·´\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab æ¯æ¬¡éƒ½éœ€è¦åŸ·è¡Œï¼‰\n",
    "# !pip install -U transformers datasets\n",
    "\n",
    "# ğŸ“Œ åŒ¯å…¥å¿…è¦å¥—ä»¶\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "\n",
    "# é¡¯ç¤º transformers å¥—ä»¶ç‰ˆæœ¬ï¼Œç¢ºèªç‰ˆæœ¬ç›¸å®¹æ€§\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# ğŸ“Œ è¼‰å…¥ç°¡è¨Šè³‡æ–™é›†ï¼ˆè«‹ç¢ºèªæª”æ¡ˆå·²ä¸Šå‚³ï¼‰\n",
    "df = pd.read_csv(\"train/train_data_0731.csv\")  # è¼‰å…¥åŒ…å« 'text' å’Œ 'label' æ¬„ä½çš„ CSV æª”æ¡ˆ\n",
    "\n",
    "# å°‡ pandas DataFrame è½‰æˆ Hugging Face çš„ Dataset æ ¼å¼\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "\n",
    "# å°‡è³‡æ–™é›†åˆ‡åˆ†æˆè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ï¼Œæ¯”ä¾‹ç‚º 80% è¨“ç·´ã€20% æ¸¬è©¦\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# ğŸ“Œ åˆå§‹åŒ– tokenizerï¼ˆä½¿ç”¨ bert-base-chineseï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# ğŸ“Œ å®šç¾© tokenize å‡½å¼ï¼šå°‡æ¯å‰‡ç°¡è¨Šè½‰æ›æˆ BERT æ¨¡å‹å¯ä»¥è®€æ‡‚çš„æ ¼å¼\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,              # è‹¥æ–‡å­—å¤ªé•·æœƒæˆªæ–·\n",
    "        padding=\"max_length\",         # è£œé½Šåˆ°å›ºå®šé•·åº¦ï¼ˆ128ï¼‰\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# å°è³‡æ–™é›†é€²è¡Œ tokenizeï¼Œç”¢ç”Ÿ token ç·¨ç¢¼ã€attention mask ç­‰æ¬„ä½\n",
    "tokenized = dataset.map(tokenize)\n",
    "\n",
    "# ğŸ“Œ è¼‰å…¥ BERT æ¨¡å‹åšäºŒåˆ†é¡ï¼ˆlabel æœ‰å…©ç¨®å¯èƒ½ï¼‰\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=2                    # äºŒåˆ†é¡ï¼šä¾‹å¦‚ æ˜¯å¦åŒ…å«äººå / è©é¨™ç­‰\n",
    ")\n",
    "\n",
    "# ğŸ“Œ å®šç¾©è¨“ç·´åƒæ•¸\n",
    "# -- TrainingArguments --------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               # å„²å­˜æ¨¡å‹èˆ‡æš«å­˜æª”çš„æ ¹è³‡æ–™å¤¾\n",
    "    per_device_train_batch_size=16,       # å–® GPU / CPU çš„ batch size\n",
    "    num_train_epochs=6,                   # è¨­è¼ƒå¤§çš„ä¸Šé™ï¼Œäº¤ç”± Early-Stopping æ±ºå®šçœŸå¯¦ epoch\n",
    "                                          \n",
    "    eval_strategy=\"epoch\",          # æ¯å€‹ epoch è·‘ä¸€æ¬¡é©—è­‰é›†  \n",
    "    save_strategy=\"epoch\",                # æ¯å€‹ epoch å„²å­˜ checkpointï¼ˆæ‰èƒ½å›æº¯æœ€ä½³æ¬Šé‡ï¼‰\n",
    "                                          \n",
    "    # ------- Logging è¨­å®š -------\n",
    "    logging_dir=\"./logs\",                 # TensorBoard/CSV log è¼¸å‡ºç›®éŒ„\n",
    "    logging_strategy=\"steps\",             # ä¾ step ç´€éŒ„è¨“ç·´ log\n",
    "    logging_steps=50,                     # æ¯ 50 step è¨˜ä¸€æ¬¡ log\n",
    "                                          \n",
    "    # ------- Best Model & Early Stopping -------\n",
    "    load_best_model_at_end=True,          # è¨“ç·´çµæŸè‡ªå‹•è¼‰å›æœ€ä½³æ¬Šé‡\n",
    "    metric_for_best_model=\"eval_loss\",    # ä»¥é©—è­‰æå¤±ä½œç‚ºæœ€ä½³æŒ‡æ¨™\n",
    "    greater_is_better=False,              # æå¤±è¶Šå°è¶Šå¥½\n",
    "                                          \n",
    "    report_to=\"none\"                      # é—œé–‰ wandb ç­‰é ç«¯ logger\n",
    ")\n",
    "\n",
    "# -- EarlyStoppingCallback ----------------------------------------------------\n",
    "early_stop = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2             # è‹¥é€£çºŒ 2 å€‹é©—è­‰é€±æœŸæŒ‡æ¨™ç„¡æ”¹å–„ï¼Œå³æå‰åœæ­¢\n",
    ")\n",
    "\n",
    "# ğŸ“Œ å®šç¾©æŒ‡æ¨™è¨ˆç®—å‡½å¼ï¼ˆaccuracy, precision, recall, F1ï¼‰\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # æ¨¡å‹é æ¸¬åˆ†æ•¸ å’Œ çœŸå¯¦æ¨™ç±¤\n",
    "    preds = logits.argmax(axis=1)  # å–æœ€å¤§åˆ†æ•¸çš„é¡åˆ¥ä½œç‚ºé æ¸¬\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')  # 2åˆ†é¡ä»»å‹™\n",
    "    acc = accuracy_score(labels, preds)  # è¨ˆç®—æº–ç¢ºç‡\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# ğŸ“Œ åˆå§‹åŒ– Trainer ç‰©ä»¶ï¼šæ•´åˆæ¨¡å‹ã€è³‡æ–™èˆ‡è¨“ç·´åƒæ•¸\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],      # è¨“ç·´é›†\n",
    "    eval_dataset=tokenized[\"test\"],         # æ¸¬è©¦é›†\n",
    "    compute_metrics=compute_metrics,        # åŠ ä¸Šé€™å€‹å‡½å¼æ‰èƒ½é¡¯ç¤ºæº–ç¢ºç‡èˆ‡ F1\n",
    "    callbacks=[early_stop]                # å•Ÿç”¨æ—©åœæ©Ÿåˆ¶\n",
    ")\n",
    "\n",
    "# ğŸ“Œ åŸ·è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()\n",
    "\n",
    "# ğŸ“Œ ä½¿ç”¨æ¸¬è©¦é›†é€²è¡Œè©•ä¼°\n",
    "results = trainer.evaluate()\n",
    "print(\"ğŸ“Š è©•ä¼°çµæœï¼š\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ğŸ“Œ å„²å­˜è¨“ç·´å®Œæˆçš„æ¨¡å‹èˆ‡ tokenizerï¼ˆæ–¹ä¾¿æœªä¾†éƒ¨ç½²æˆ–è¼‰å…¥ä½¿ç”¨ï¼‰\n",
    "model.save_pretrained(\"./results/final\")\n",
    "tokenizer.save_pretrained(\"./results/final\")\n",
    "\n",
    "# ğŸ“Œ å°‡å„²å­˜çš„æ¨¡å‹æ‰“åŒ…æˆå£“ç¸®æª”ï¼Œæ–¹ä¾¿ä¸‹è¼‰\n",
    "!zip -r model_results.zip ./results/final\n",
    "\n",
    "print(\"å·²å®Œæˆè¨“ç·´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP70pwkffXXTy0PTlKZzhAc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
