{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWBvabbRT_BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "4.53.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0138dc56cb4ba297dfb7f9ffcbeaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e193de20d87148faa160f45e0171863c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 07:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.997895</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/charleswang/Documents/sms-classification/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è©•ä¼°çµæœï¼š\n",
      "eval_loss: 0.0087\n",
      "eval_accuracy: 0.9979\n",
      "eval_precision: 0.9954\n",
      "eval_recall: 1.0000\n",
      "eval_f1: 0.9977\n",
      "eval_runtime: 9.8179\n",
      "eval_samples_per_second: 48.3810\n",
      "eval_steps_per_second: 6.1110\n",
      "epoch: 3.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/final/ (stored 0%)\n",
      "  adding: results/final/model.safetensors (deflated 7%)\n",
      "  adding: results/final/tokenizer_config.json (deflated 75%)\n",
      "  adding: results/final/special_tokens_map.json (deflated 42%)\n",
      "  adding: results/final/config.json (deflated 54%)\n",
      "  adding: results/final/tokenizer.json (deflated 75%)\n",
      "  adding: results/final/vocab.txt (deflated 48%)\n",
      "å·²å®Œæˆè¨“ç·´\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab æ¯æ¬¡éƒ½éœ€è¦åŸ·è¡Œï¼‰\n",
    "# !pip install -U transformers datasets\n",
    "\n",
    "# ğŸ“Œ åŒ¯å…¥å¿…è¦å¥—ä»¶\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "\n",
    "# é¡¯ç¤º transformers å¥—ä»¶ç‰ˆæœ¬ï¼Œç¢ºèªç‰ˆæœ¬ç›¸å®¹æ€§\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# ğŸ“Œ è¼‰å…¥ç°¡è¨Šè³‡æ–™é›†ï¼ˆè«‹ç¢ºèªæª”æ¡ˆå·²ä¸Šå‚³ï¼‰\n",
    "df = pd.read_csv(\"train/train_data_0711_2.csv\")  # è¼‰å…¥åŒ…å« 'text' å’Œ 'label' æ¬„ä½çš„ CSV æª”æ¡ˆ\n",
    "\n",
    "# å°‡ pandas DataFrame è½‰æˆ Hugging Face çš„ Dataset æ ¼å¼\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "\n",
    "# å°‡è³‡æ–™é›†åˆ‡åˆ†æˆè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ï¼Œæ¯”ä¾‹ç‚º 80% è¨“ç·´ã€20% æ¸¬è©¦\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# ğŸ“Œ åˆå§‹åŒ– tokenizerï¼ˆä½¿ç”¨ bert-base-chineseï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# ğŸ“Œ å®šç¾© tokenize å‡½å¼ï¼šå°‡æ¯å‰‡ç°¡è¨Šè½‰æ›æˆ BERT æ¨¡å‹å¯ä»¥è®€æ‡‚çš„æ ¼å¼\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,              # è‹¥æ–‡å­—å¤ªé•·æœƒæˆªæ–·\n",
    "        padding=\"max_length\",         # è£œé½Šåˆ°å›ºå®šé•·åº¦ï¼ˆ128ï¼‰\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# å°è³‡æ–™é›†é€²è¡Œ tokenizeï¼Œç”¢ç”Ÿ token ç·¨ç¢¼ã€attention mask ç­‰æ¬„ä½\n",
    "tokenized = dataset.map(tokenize)\n",
    "\n",
    "# ğŸ“Œ è¼‰å…¥ BERT æ¨¡å‹åšäºŒåˆ†é¡ï¼ˆlabel æœ‰å…©ç¨®å¯èƒ½ï¼‰\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=2                    # äºŒåˆ†é¡ï¼šä¾‹å¦‚ æ˜¯å¦åŒ…å«äººå / è©é¨™ç­‰\n",
    ")\n",
    "\n",
    "# ğŸ“Œ å®šç¾©è¨“ç·´åƒæ•¸\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # æ¨¡å‹èˆ‡æª”æ¡ˆè¼¸å‡ºç›®éŒ„\n",
    "    per_device_train_batch_size=16,         # æ¯å€‹è¨­å‚™çš„è¨“ç·´æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs=3,                     # è¨“ç·´é€±æœŸæ•¸\n",
    "    eval_strategy=\"epoch\",                 # æ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡æ¸¬è©¦é›†\n",
    "    save_strategy=\"epoch\",                 # æ¯å€‹ epoch å„²å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    logging_dir=\"./logs\",                  # è¨“ç·´éç¨‹ log è¼¸å‡ºè³‡æ–™å¤¾\n",
    "    load_best_model_at_end=True,           # è¨“ç·´çµæŸæ™‚è¼‰å…¥æœ€å¥½çš„æ¨¡å‹ï¼ˆæ ¹æ“š eval lossï¼‰\n",
    "    report_to=\"none\"                       # ä¸ä¸Šå‚³è¨“ç·´è¨˜éŒ„åˆ° wandb\n",
    ")\n",
    "\n",
    "# ğŸ“Œ å®šç¾©æŒ‡æ¨™è¨ˆç®—å‡½å¼ï¼ˆaccuracy, precision, recall, F1ï¼‰\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # æ¨¡å‹é æ¸¬åˆ†æ•¸ å’Œ çœŸå¯¦æ¨™ç±¤\n",
    "    preds = logits.argmax(axis=1)  # å–æœ€å¤§åˆ†æ•¸çš„é¡åˆ¥ä½œç‚ºé æ¸¬\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')  # 2åˆ†é¡ä»»å‹™\n",
    "    acc = accuracy_score(labels, preds)  # è¨ˆç®—æº–ç¢ºç‡\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# ğŸ“Œ åˆå§‹åŒ– Trainer ç‰©ä»¶ï¼šæ•´åˆæ¨¡å‹ã€è³‡æ–™èˆ‡è¨“ç·´åƒæ•¸\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],      # è¨“ç·´é›†\n",
    "    eval_dataset=tokenized[\"test\"],         # æ¸¬è©¦é›†\n",
    "    compute_metrics=compute_metrics        # åŠ ä¸Šé€™å€‹å‡½å¼æ‰èƒ½é¡¯ç¤ºæº–ç¢ºç‡èˆ‡ F1\n",
    ")\n",
    "\n",
    "# ğŸ“Œ åŸ·è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()\n",
    "\n",
    "# ğŸ“Œ ä½¿ç”¨æ¸¬è©¦é›†é€²è¡Œè©•ä¼°\n",
    "results = trainer.evaluate()\n",
    "print(\"ğŸ“Š è©•ä¼°çµæœï¼š\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ğŸ“Œ å„²å­˜è¨“ç·´å®Œæˆçš„æ¨¡å‹èˆ‡ tokenizerï¼ˆæ–¹ä¾¿æœªä¾†éƒ¨ç½²æˆ–è¼‰å…¥ä½¿ç”¨ï¼‰\n",
    "model.save_pretrained(\"./results/final\")\n",
    "tokenizer.save_pretrained(\"./results/final\")\n",
    "\n",
    "# ğŸ“Œ å°‡å„²å­˜çš„æ¨¡å‹æ‰“åŒ…æˆå£“ç¸®æª”ï¼Œæ–¹ä¾¿ä¸‹è¼‰\n",
    "!zip -r model_results.zip ./results/final\n",
    "\n",
    "print(\"å·²å®Œæˆè¨“ç·´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP70pwkffXXTy0PTlKZzhAc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
